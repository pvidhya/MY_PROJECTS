{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Machine Learning Project - Crime Prediction</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import pydot\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier,export_graphviz\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model.ridge import Ridge,RidgeCV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting files from the zip folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract():\n",
    "    fh = open('Crime Prediction Data.zip', 'rb')\n",
    "    z = zipfile.ZipFile(fh)\n",
    "    for name in z.namelist():\n",
    "        z.extract(name)\n",
    "    fh.close()\n",
    "extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>DECISION TREES</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. a) Calculating the percentage of positive and negative instances in the clean dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of positive instances:62.719518314099346\n",
      "Percentage of negative instances:37.280481685900654\n"
     ]
    }
   ],
   "source": [
    "crime_clean = pd.read_csv('Crime Prediction Data/communities-crime-clean.csv')\n",
    "highCrime=[]\n",
    "true_cnt=0\n",
    "for x in crime_clean['ViolentCrimesPerPop']:\n",
    "    if x>0.1:\n",
    "        highCrime.append('True')\n",
    "        true_cnt+=1\n",
    "    else:\n",
    "        highCrime.append('False')\n",
    "#Creating the new column highCrime and appending to existing document\n",
    "crime_clean['highCrime']=pd.Series(highCrime)\n",
    "crime_clean.to_csv('Crime Prediction Data/communities-crime-clean.csv', sep=',', encoding='utf-8')\n",
    "print('Percentage of positive instances:'+str((true_cnt/len(crime_clean['ViolentCrimesPerPop']))*100))\n",
    "print('Percentage of negative instances:'+str(((len(crime_clean['ViolentCrimesPerPop'])-true_cnt)/len(crime_clean['ViolentCrimesPerPop']))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = list(crime_clean.columns[:len(crime_clean.columns)-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx=0\n",
    "comm={}\n",
    "community_name=[]\n",
    "for x in crime_clean['communityname']:\n",
    "    if x not in comm:\n",
    "        comm[x]=idx\n",
    "        idx+=1\n",
    "    community_name.append(comm[x])\n",
    "crime_clean['communityname']=pd.Series(community_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Decision Tree Classifier</u>\n",
    "\n",
    "The minimum number of samples required to split an internal node is kept as 20 to avoid overfitting\n",
    "\n",
    "Saving the labels, gini values, samples and values into dt.dot file.\n",
    "\n",
    "Plotting the decision tree from the data saved in dt.dot file on the basis of gini index and information gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = crime_clean[\"highCrime\"]\n",
    "X = crime_clean[features]\n",
    "dt = DecisionTreeClassifier(min_samples_split=20, random_state=99)\n",
    "dt.fit(X, y)\n",
    "#Requires graphviz to print the decision tree\n",
    "with open(\"dt.dot\", 'w') as f:\n",
    "    export_graphviz(dt, out_file=f,feature_names=features)\n",
    "\n",
    "\n",
    "command = [\"dot\", \"-Tpng\", \"dt.dot\", \"-o\", \"dt.png\"]\n",
    "try:\n",
    "    subprocess.check_call(command)\n",
    "except:\n",
    "    exit(\"Could not run dot, ie graphviz, to produce visualization\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"dt.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. b) i. Calculating the training Accuracy, Precision, and Recall for the plotted decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9357752132463623\n",
      "precision: 0.946656050955414\n",
      "recall: 0.9512\n"
     ]
    }
   ],
   "source": [
    "ypredict=dt.predict(X)\n",
    "\n",
    "TP=0\n",
    "TN=0\n",
    "FP=0\n",
    "FN=0\n",
    "\n",
    "for x in zip(ypredict,y):\n",
    "    if(x[0]=='True' and x[1]=='True'):\n",
    "        TP+=1\n",
    "    elif x[0]=='False' and x[1]=='False':\n",
    "        TN+=1\n",
    "    elif x[0]=='True' and x[1]=='False':\n",
    "        FP+=1\n",
    "    elif x[0]=='False' and x[1]=='True':\n",
    "        FN+=1\n",
    "\n",
    "acc=(TP + TN)/(TP + TN + FP + FN)\n",
    "pre=TP/(TP + FP)\n",
    "rec=TP/(TP + FN)\n",
    "\n",
    "print('accuracy:',acc)\n",
    "print('precision:',pre)\n",
    "print('recall:',rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. b) ii. Finding the main features used for the decision tree classification.\n",
    "\n",
    "The features with higher importance value are treated as the main features.\n",
    "\n",
    "We are printing the top 10 features that contribute the most in decision making.\n",
    "\n",
    "This makes sense as these are the features that contribute the maximum information gain for the splitting in the decision tree. The gini index is used to calculate the feature importance field which inturn is also a measure of information gain achieved by splitting through a particular feature at a particular instance in decision tree formation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('PctKids2Par', 0.44176010471803651), ('racePctWhite', 0.10649545734925057), ('racePctHisp', 0.059843685126445598), ('PctEmplManu', 0.020406385475740262), ('HousVacant', 0.01755812277747543), ('communityname', 0.016744977095863738), ('HispPerCap', 0.013349381238338801), ('MedOwnCostPctInc', 0.012869430272866069), ('blackPerCap', 0.012404392394867894), ('state', 0.011674778724944698)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(list(zip(features,dt.feature_importances_)),key=lambda x: -x[1])[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Cross Val Score</u>\n",
    "1. c) i. Calculating the 10-fold cross validation Accuracy, Precision and Recall for the above decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:0.716487437186\n",
      "precision:0.78942069483\n",
      "recall:0.7648\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(min_samples_split=20, random_state=99)\n",
    "print('accuracy:',end=\"\")\n",
    "print(np.mean(cross_val_score(dt, X, y, cv=10, scoring='accuracy')))\n",
    "ybin=[]\n",
    "for x in y:\n",
    "    if x=='True':\n",
    "        ybin.append(1)\n",
    "    else:\n",
    "        ybin.append(0)\n",
    "print('precision:',end=\"\")\n",
    "print(np.mean(cross_val_score(dt, X, ybin, cv=10, scoring='precision')))\n",
    "print('recall:',end=\"\")\n",
    "print(np.mean(cross_val_score(dt, X, ybin, cv=10, scoring='recall')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. c) ii.Why are they different from the results in the previous test?\n",
    "\n",
    "The cross validation does not train the model on entire dataset. It partition the data as training and testing sections. The train data fits the model and test data is used to check the model properties. Since the sample count available to the model is reduced and new unseen samples need to be predicted(test data), the model accuracy, precision and recall might decrease based on the correlation between the samples.\n",
    "    The previous test was conducted on the very same data which was used to train the model and thus there is no unseen data to be predicted.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>Linear Classification</u>\n",
    "\n",
    "### <u>Gausian Naive Bayes</u>\n",
    "2.a)ii.Calculating the 10-fold cross validation Accuracy, Precision and Recall for the Gaussian Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:0.759600502513\n",
      "precision:0.913411959617\n",
      "recall:0.6872\n"
     ]
    }
   ],
   "source": [
    "gaus_full = GaussianNB()\n",
    "gaus_full.fit(X, ybin)\n",
    "\n",
    "ypredict = gaus_full.predict(X)\n",
    "gaus = GaussianNB()\n",
    "print('accuracy:',end=\"\")\n",
    "print(np.mean(cross_val_score(gaus, X, ybin, cv=10, scoring='accuracy')))\n",
    "print('precision:',end=\"\")\n",
    "print(np.mean(cross_val_score(gaus, X, ybin, cv=10, scoring='precision')))\n",
    "print('recall:',end=\"\")\n",
    "print(np.mean(cross_val_score(gaus, X, ybin, cv=10, scoring='recall')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.a) ii. Finding the 10 most predictive features. \n",
    "\n",
    "The larger this different, the more predictive the feature. Why do these make sense (or not)?\n",
    "\n",
    "The larger the difference between mean value of each class for a feature, it is easier to predict the class of the feature. If they are close the margin of choosing the class become tedious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('PctKids2Par', 4.9465652122871377), ('FemalePctDiv', 4.6954189323204307), ('PctFam2Par', 4.5446144318138764), ('pctWInvInc', 4.3415293287236265), ('TotalPctDiv', 4.3349519037702082), ('PctTeen2Par', 3.9552407232282452), ('MalePctDivorce', 3.9060868313950698), ('PctYoungKids2Par', 3.6166737824465711), ('PctIlleg', 3.4411529842773367), ('racePctWhite', 3.4174474764677996)]\n"
     ]
    }
   ],
   "source": [
    "uT=np.array(gaus_full.theta_[0])\n",
    "uF=np.array(gaus_full.theta_[1])\n",
    "sT=np.array(gaus_full.sigma_[0])\n",
    "sF=np.array(gaus_full.sigma_[1])\n",
    "\n",
    "fpredict=abs(uT-uF)/(sT+sF)\n",
    "\n",
    "print(sorted(list(zip(features,fpredict)),key=lambda x: -x[1])[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 2.a) iii.\tHow do these results compare with your results from decision trees, above?\n",
    "\n",
    "The Accuracy and Precision of the samples by Gaussian Naive Bayes is higher than higher than the results from decision tree method. But the Recall value is much lower than the desicion tree classification.\n",
    "\n",
    "Gausian Naive Bayes works better when there is less samples to train. Decision Trees are generally efficient when more data is available. So the cross validation result for Accuracy and Precision sfavors naive bayes. Also decision trees tend to overfit the data and we have not done pruning to reduce it either in the above case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Linear Support Vector Machine</u>\n",
    "\n",
    "2.b) i. Calculating the 10-fold cross validation Accuracy, Precision and Recall for LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:0.583914572864\n",
      "precision:0.764382064848\n",
      "recall:0.7352\n"
     ]
    }
   ],
   "source": [
    "svc_full = LinearSVC()\n",
    "svc_full.fit(X, ybin)\n",
    "\n",
    "ypredict = svc_full.predict(X)\n",
    "\n",
    "svc = LinearSVC()\n",
    "print('accuracy:',end=\"\")\n",
    "print(np.mean(cross_val_score(svc, X, ybin, cv=10, scoring='accuracy')))\n",
    "print('precision:',end=\"\")\n",
    "print(np.mean(cross_val_score(svc, X, ybin, cv=10, scoring='precision')))\n",
    "print('recall:',end=\"\")\n",
    "print(np.mean(cross_val_score(svc, X, ybin, cv=10, scoring='recall')))\n",
    "\n",
    "f_weight=svc_full.coef_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.b) ii. Finding the 10 most predictive features. How does this make sense?\n",
    "\n",
    "If the svm finds one feature useful for separating the data, then the hyperplane would be orthogonal to that axis. So, you could say that the absolute size of the coefficient relative to the other ones gives an indication of how important the feature was for the separation. This theory is used for the weights present in the coefficient attribute of LinearSVC library function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('racepctblack', 0.91767556333070233), ('racePctWhite', 0.8210632064789174), ('MedYrHousBuilt', 0.80907214154033869), ('pctWFarmSelf', 0.63067900474089333), ('AsianPerCap', 0.59672428694009905), ('PctVacMore6Mos', 0.53760186266049614), ('MedNumBR', 0.51040296636928739), ('PctWOFullPlumb', 0.50866176984957512), ('PctWorkMomYoungKids', 0.48929668515186997), ('PctSameCity85', 0.48588876271041015)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(list(zip(features,np.absolute(f_weight))),key=lambda x: -x[1])[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.b) iii.\tHow do these results compare with your results from decision trees, above?\n",
    "\n",
    "The Precision and Recall of the samples by SVC method is higher than the results from decision tree method.\n",
    "\n",
    "But the Accuracy value is much lower than the desicion tree classification.\n",
    "\n",
    "We are using a linear kernel svm with L2 regularization which is not the highly efficient way nor the complex way of classification. If the data set is in such way that the linear plane can't split the data into various classes, the efficiency of this method is at stake. May be with a better kernel (non linear) the accuracy may be improved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Regression</u>\n",
    "\n",
    "## <u>Linear Regression</u>\n",
    "Directly predicting the Crime rate per capita.\n",
    "\n",
    "3.a) ii.Calculating the estimated mean squared error by Linear Regression model (training on all the sample data and then testing on all of them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error:0.115663412575\n"
     ]
    }
   ],
   "source": [
    "lm_full = LinearRegression()\n",
    "lm_full.fit(X, ybin)\n",
    "\n",
    "ypredict=lm_full.predict(X)\n",
    "\n",
    "print(\"Mean Squared Error:\",end=\"\")\n",
    "print(np.mean((ypredict-ybin)**2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.a) i. Calculating the mean squared error for 10 fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Fold Cross Val MSE:0.132441066083\n"
     ]
    }
   ],
   "source": [
    "lm = LinearRegression()\n",
    "print(\"10 Fold Cross Val MSE:\",end=\"\")\n",
    "print(np.mean(cross_val_score(lm_full, X, ybin, cv=10, scoring='neg_mean_squared_error'))*-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.a) iii. Finding the features that are most predictive of high crime rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best features(High Crime Rate):[('population', 3.8997444222376658), ('PersPerOccupHous', 1.196471116775389), ('medFamInc', 0.92585261186539869), ('PctRecImmig8', 0.74239608782952038), ('PctOccupMgmtProf', 0.72483250979751523), ('MalePctDivorce', 0.65950634306271172), ('RentHighQ', 0.64678614475478835), ('PctRecImmig5', 0.59612562572439343), ('PctHousOwnOcc', 0.51592477449600938), ('FemalePctDiv', 0.5031651871844991)]\n"
     ]
    }
   ],
   "source": [
    "print(\"best features(High Crime Rate):\",end=\"\")\n",
    "print(sorted(list(zip(features,lm_full.coef_)),key=lambda x: -x[1])[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best features(Low Crime Rate):[('numbUrban', -2.7818491648335746), ('PctRecImmig10', -1.2374823539527968), ('PctKids2Par', -1.2159286728821308), ('medIncome', -1.0131426320284938), ('pctWInvInc', -0.93624836910933107), ('NumIlleg', -0.84323649082412888), ('TotalPctDiv', -0.77387187167161109), ('PctRecentImmig', -0.58270110254631569), ('MedRent', -0.57298131417125142), ('PctBSorMore', -0.47860448618568341)]\n"
     ]
    }
   ],
   "source": [
    "print(\"best features(Low Crime Rate):\",end=\"\")\n",
    "print(sorted(list(zip(features,lm_full.coef_)),key=lambda x: x[1])[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## <u>Ridge Regression</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.b) i.Calucating the mean squared error using RidgeCV model under 10-fold CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Fold Cross Val MSE:0.131520953897\n"
     ]
    }
   ],
   "source": [
    "rdcv=RidgeCV(alphas=(10, 1, 0.1, 0.01, 0.001),cv=10)\n",
    "\n",
    "print(\"10 Fold Cross Val MSE:\",end=\"\")\n",
    "print(np.mean(cross_val_score(rdcv, X, ybin, cv=10, scoring='neg_mean_squared_error'))*-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.b) ii. Calucating the mean squared error using RidgeCV model by training on all the sample data and testing on all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on training set:0.117495628407\n"
     ]
    }
   ],
   "source": [
    "rdcv.fit(X,ybin)\n",
    "rd=Ridge(alpha=rdcv.alpha_)\n",
    "rd.fit(X,ybin)\n",
    "ypredict=rd.predict(X)\n",
    "print(\"Mean Squared Error on training set:\",end=\"\")\n",
    "print(np.mean((ypredict-ybin)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.b) iii. Calculating the Best Alpha among the given values using RidgeCV model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Best alpha:\",rdcv.alpha_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.b) iv.\tWhat does this say about the amount of overfitting in linear regression for this problem?\n",
    "\n",
    "A regularized linear regression model is Ridge Regression. This adds the L2 norm of the coefficients to the ordinary least squares objective. If alpha is 0, the coefficients won't be penalized and is similar to linear regression. \n",
    "\n",
    "AS value of alpha increases, the model complexity reduces. Though higher values of alpha reduce overfitting, significantly high values can cause underfitting as well. Since our alpha is not between 0 and 1, the coefficient weights get decreased i.e. it reduces overfitting.\n",
    "\n",
    "OVERFITTING REDUCED.............!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>Polynomial Features</u>\n",
    "\n",
    "3.c) i. Calculating the mean squared error using Polynomial Regression model under 10-fold CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Fold Cross Val MSE:1.24362859175\n"
     ]
    }
   ],
   "source": [
    "poly=PolynomialFeatures(2)\n",
    "X_new=poly.fit_transform(X,ybin)\n",
    "\n",
    "\n",
    "lm_full=LinearRegression()\n",
    "lm_full.fit(X_new,ybin)\n",
    "ypredict==lm_full.predict(X_new)\n",
    "\n",
    "lm=LinearRegression()\n",
    "print(\"10 Fold Cross Val MSE:\",end=\"\")\n",
    "print(np.mean(cross_val_score(lm, X_new, ybin, cv=10, scoring='neg_mean_squared_error'))*-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.c) ii. Calucating the mean squared error using Polynomial Regression model by training on all the sample data and testing on all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error:0.117495628407\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean Squared Error:\",end=\"\")\n",
    "print(np.mean((ypredict.flatten()-ybin)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MSE under 10-fold CV for Polynomial Regression model is greater than that of the Linear Model. This means that for the given sample of data Linear Model is better as compared to the Polynomial Regression.\n",
    "\n",
    "The MSE value of training on all the samples and testing on all of them is greater for polynomial regression. The polynomial regression with degree 2 tend to overfit the data. So the linear model is better in this case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>DIRTY DATA</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "crime_full = pd.read_csv('Crime Prediction Data/communities-crime-full.csv',sep=',', na_values=[\"?\"])\n",
    "highCrime=[]\n",
    "true_cnt=0\n",
    "for x in crime_full['ViolentCrimesPerPop']:\n",
    "    if x>0.1:\n",
    "        highCrime.append('True')\n",
    "        true_cnt+=1\n",
    "    else:\n",
    "        highCrime.append('False')\n",
    "crime_full['highCrime']=pd.Series(highCrime)\n",
    "crime_full.to_csv('Crime Prediction Data/communities-crime-full.csv', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = list(crime_full.columns[:len(crime_full.columns)-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx=0\n",
    "comm={}\n",
    "community_name=[]\n",
    "for x in crime_full['communityname']:\n",
    "    if x not in comm:\n",
    "        comm[x]=idx\n",
    "        idx+=1\n",
    "    community_name.append(comm[x])\n",
    "crime_full['communityname']=pd.Series(community_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the labels, gini values, samples and vaues into ddt.dot file.\n",
    "\n",
    "Plotting the decision tree from the data saved in dt.dot file on the basis of gini index and information gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = crime_full[\"highCrime\"]\n",
    "X = crime_full[features]\n",
    "X.is_copy = False\n",
    "\n",
    "\n",
    "# filling the missing values with mean\n",
    "fill_NaN = Imputer(missing_values=np.nan, strategy='mean')\n",
    "X = pd.DataFrame(fill_NaN.fit_transform(X))\n",
    "# fill the missing values with zero commented\n",
    "# X.fillna(0,inplace=True)\n",
    "\n",
    "dt = DecisionTreeClassifier(min_samples_split=20, random_state=99)\n",
    "dt.fit(X, y)\n",
    "\n",
    "with open(\"ddt.dot\", 'w') as f:\n",
    "    export_graphviz(dt, out_file=f,feature_names=features)\n",
    "\n",
    "\n",
    "command = [\"dot\", \"-Tpng\", \"ddt.dot\", \"-o\", \"ddt.png\"]\n",
    "try:\n",
    "    subprocess.check_call(command)\n",
    "except:\n",
    "    exit(\"Could not run dot, ie graphviz, to produce visualization\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the training Accuracy, Precision, and Recall for the plotted decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9433299899699097\n",
      "precision: 0.9473270440251572\n",
      "recall: 0.9632294164668266\n"
     ]
    }
   ],
   "source": [
    "ypredict=dt.predict(X)\n",
    "\n",
    "TP=0\n",
    "TN=0\n",
    "FP=0\n",
    "FN=0\n",
    "\n",
    "for x in zip(ypredict,y):\n",
    "    if(x[0]=='True' and x[1]=='True'):\n",
    "        TP+=1\n",
    "    elif x[0]=='False' and x[1]=='False':\n",
    "        TN+=1\n",
    "    elif x[0]=='True' and x[1]=='False':\n",
    "        FP+=1\n",
    "    elif x[0]=='False' and x[1]=='True':\n",
    "        FN+=1\n",
    "\n",
    "acc=(TP + TN)/(TP + TN + FP + FN)\n",
    "pre=TP/(TP + FP)\n",
    "rec=TP/(TP + FN)\n",
    "\n",
    "print('accuracy:',acc)\n",
    "print('precision:',pre)\n",
    "print('recall:',rec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the main features used for the decision tree classification.\n",
    "\n",
    "The features with higher importance value are treated as the main features.\n",
    "\n",
    "We are printing the top 10 features that contribute the most in decision making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('PctKids2Par', 0.42698574130639683), ('racePctWhite', 0.10414025388676976), ('racePctHisp', 0.054317738929504394), ('communityname', 0.021353588305276783), ('HousVacant', 0.020309976203017808), ('PctEmplManu', 0.014656348313214289), ('blackPerCap', 0.014527159561124818), ('HispPerCap', 0.012827799541958205), ('PctHousOwnOcc', 0.012141343625738019), ('PctNotHSGrad', 0.012111657460089353)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(list(zip(features,dt.feature_importances_)),key=lambda x: -x[1])[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the 10-fold cross validation Accuracy, Precision and Recall for the above decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:0.762255281382\n",
      "precision:0.821636589773\n",
      "recall:0.79453968254\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(min_samples_split=20, random_state=99)\n",
    "print('accuracy:',end=\"\")\n",
    "print(np.mean(cross_val_score(dt, X, y, cv=10, scoring='accuracy')))\n",
    "ybin=[]\n",
    "for x in y:\n",
    "    if x=='True':\n",
    "        ybin.append(1)\n",
    "    else:\n",
    "        ybin.append(0)\n",
    "print('precision:',end=\"\")\n",
    "print(np.mean(cross_val_score(dt, X, ybin, cv=10, scoring='precision')))\n",
    "print('recall:',end=\"\")\n",
    "print(np.mean(cross_val_score(dt, X, ybin, cv=10, scoring='recall')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CV results for Accuracy, Precision and Recall are greater for Dirty data(full file) than the communities-crime-clean.csv file even though there are many missing values in the communities-crime-full.csv file.\n",
    "\n",
    "This happens because in this method we are imputing the missing values with the mean values of the feature. \n",
    "\n",
    "Since the mean values are almost higher than the original sample values, the CV results on this file gives better result in terms of Accuracy, Precision and Recall.\n",
    "\n",
    "The count of missing values in few of the columns is comparatively low against the entire dataset and it makes very little effect on training the model. If the missing data is sufficiently high, the model efficiency would be clearly affected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Team Extras - 2 Members"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K nearest neighbor\n",
    "\n",
    "This is one of the classification we learnt in the course work. The euclidean distance between samples is considered as the similarity measure between two records in this model. Here we implement KNN with n(neighbors) values as 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean file\n",
      "accuracy:0.283376884422\n",
      "precision:0.405180128929\n",
      "recall:0.3384\n",
      "Full file\n",
      "accuracy:0.673008550214\n",
      "precision:0.733024607316\n",
      "recall:0.75373968254\n"
     ]
    }
   ],
   "source": [
    "features_clean = list(crime_clean.columns[:len(crime_clean.columns)-2])\n",
    "features_full = list(crime_full.columns[:len(crime_full.columns)-2])\n",
    "X_clean = crime_clean[features_clean]\n",
    "y_clean= crime_clean[\"highCrime\"]\n",
    "ybin_clean=[]\n",
    "for x in y_clean:\n",
    "    if x=='True':\n",
    "        ybin_clean.append(1)\n",
    "    else:\n",
    "        ybin_clean.append(0)\n",
    "\n",
    "X_full = crime_full[features]\n",
    "y_full = crime_full[\"highCrime\"]\n",
    "X_full.is_copy = False\n",
    "ybin_full=[]\n",
    "for x in y_full:\n",
    "    if x=='True':\n",
    "        ybin_full.append(1)\n",
    "    else:\n",
    "        ybin_full.append(0)\n",
    "\n",
    "\n",
    "# filling the missing values with mean\n",
    "fill_NaN = Imputer(missing_values=np.nan, strategy='mean')\n",
    "X_full = pd.DataFrame(fill_NaN.fit_transform(X_full))\n",
    "\n",
    "\n",
    "#fitting for clean file\n",
    "knn_clean=KNeighborsClassifier(n_neighbors=20)\n",
    "print(\"Clean file\")\n",
    "print('accuracy:',end=\"\")\n",
    "print(np.mean(cross_val_score(knn_clean, X_clean, ybin_clean, cv=10, scoring='accuracy')))\n",
    "print('precision:',end=\"\")\n",
    "print(np.mean(cross_val_score(knn_clean, X_clean, ybin_clean, cv=10, scoring='precision')))\n",
    "print('recall:',end=\"\")\n",
    "print(np.mean(cross_val_score(knn_clean, X_clean, ybin_clean, cv=10, scoring='recall')))\n",
    "\n",
    "\n",
    "\n",
    "#fitting for full file\n",
    "knn_full=KNeighborsClassifier(n_neighbors=20)\n",
    "print(\"Full file\")\n",
    "print('accuracy:',end=\"\")\n",
    "print(np.mean(cross_val_score(knn_full, X_full, ybin_full, cv=10, scoring='accuracy')))\n",
    "print('precision:',end=\"\")\n",
    "print(np.mean(cross_val_score(knn_full, X_full, ybin_full, cv=10, scoring='precision')))\n",
    "print('recall:',end=\"\")\n",
    "print(np.mean(cross_val_score(knn_full, X_full, ybin_full, cv=10, scoring='recall')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis\n",
    "\n",
    "The basic idea of LDA is, for each class to be identified, calculate a linear function of the attributes. The class function yielding the highest score represents the predicted class. LDA bears some resemblance to principal components analysis (PCA) which made us choose this classifier for prediction. After projection of the data on the linear discriminant dimension, a classification threshold is placed at the midpoint between the two class means. This is equivalent to placing a decision hyperplane orthogonal to the discriminant dimension in response pattern space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean file\n",
      "accuracy:0.787716080402\n",
      "precision:0.838948220569\n",
      "recall:0.8344\n",
      "Full file\n",
      "accuracy:0.822964074102\n",
      "precision:0.859766972476\n",
      "recall:0.858520634921\n"
     ]
    }
   ],
   "source": [
    "#fitting for clean file\n",
    "warnings.filterwarnings('ignore')\n",
    "lda_clean=LinearDiscriminantAnalysis()\n",
    "print(\"Clean file\")\n",
    "print('accuracy:',end=\"\")\n",
    "print(np.mean(cross_val_score(lda_clean, X_clean, ybin_clean, cv=10, scoring='accuracy')))\n",
    "print('precision:',end=\"\")\n",
    "print(np.mean(cross_val_score(lda_clean, X_clean, ybin_clean, cv=10, scoring='precision')))\n",
    "print('recall:',end=\"\")\n",
    "print(np.mean(cross_val_score(lda_clean, X_clean, ybin_clean, cv=10, scoring='recall')))\n",
    "\n",
    "\n",
    "\n",
    "#fitting for full file\n",
    "lda_full=LinearDiscriminantAnalysis()\n",
    "print(\"Full file\")\n",
    "print('accuracy:',end=\"\")\n",
    "print(np.mean(cross_val_score(lda_full, X_full, ybin_full, cv=10, scoring='accuracy')))\n",
    "print('precision:',end=\"\")\n",
    "print(np.mean(cross_val_score(lda_full, X_full, ybin_full, cv=10, scoring='precision')))\n",
    "print('recall:',end=\"\")\n",
    "print(np.mean(cross_val_score(lda_full, X_full, ybin_full, cv=10, scoring='recall')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii.\tWhat method gives the best results?\n",
    "   \n",
    "   The linear discriminant analysis have better accuracy , precision and recall over the K nearest neighbor. So LDA is best classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iii.What feature(s) seem to be most consistently predictive of high crime rates? How reliable is this conclusion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best features in clean file\n",
      "[('population', 33.682483458062947), ('numbUrban', 24.027110069840454), ('PctRecImmig10', 10.688259127697631), ('PctKids2Par', 10.502097823910718), ('PersPerOccupHous', 10.334040961529821), ('medIncome', 8.7506144631829628), ('pctWInvInc', 8.0864710070066419), ('medFamInc', 7.99668180968537), ('NumIlleg', 7.2831180913914313), ('TotalPctDiv', 6.6840089231070472)]\n",
      "Best features in full file\n",
      "[('population', 34.304785589750345), ('LemasSwFTPerPop', 33.299263703943552), ('PolicPerPop', 27.560157633147412), ('numbUrban', 25.259002184021409), ('LemasSwFTFieldOps', 12.361383605515076), ('LemasSwornFT', 11.638960241056196), ('PersPerOccupHous', 10.999772290326963), ('PctRecImmig10', 10.626951764114063), ('PctKids2Par', 10.197234171221925), ('medIncome', 8.737217384745037)]\n"
     ]
    }
   ],
   "source": [
    "lda_clean.fit(X_clean,ybin_clean)\n",
    "lda_full.fit(X_full,ybin_full)\n",
    "\n",
    "\n",
    "print(\"Best features in clean file\")\n",
    "print(sorted(list(zip(features_clean,np.absolute(lda_clean.coef_[0]))),key=lambda x: -x[1])[:10])\n",
    "\n",
    "print(\"Best features in full file\")\n",
    "print(sorted(list(zip(features_full,np.absolute(lda_full.coef_[0]))),key=lambda x: -x[1])[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight coefficient of features provide the magnitude of relevance of itself in the orthogonal hyperplane. So the absolute value is the measure of its importance with respect to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Extra Credit</u>\n",
    "\n",
    "## <u>Random Forest</u>\n",
    "Random forest is an extension of bagged decision trees. Bagging means building multiple models (typically of the same type) from different subsamples of the training dataset.\n",
    "\n",
    "It is an ensemble methods in which a group of “weak learners,i.e, \"decision trees\", come together to form a “strong learner”,i.e, \"random forest\". Each classifier, individually, is a “weak learner,” while all the classifiers taken together are a “strong learner”. A random subset of  features are considered for each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:0.785203517588\n",
      "precision:0.828130998879\n",
      "recall:0.8488\n"
     ]
    }
   ],
   "source": [
    "rdm = RandomForestClassifier(n_estimators=10, criterion='gini', max_depth=None, min_samples_split=20, \n",
    "min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_split=1e-07, bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False, class_weight=None)\n",
    "\n",
    "print('accuracy:',end=\"\")\n",
    "print(np.mean(cross_val_score(rdm, X_clean, ybin_clean, cv=10, scoring='accuracy')))\n",
    "print('precision:',end=\"\")\n",
    "print(np.mean(cross_val_score(rdm, X_clean, ybin_clean, cv=10, scoring='precision')))\n",
    "print('recall:',end=\"\")\n",
    "print(np.mean(cross_val_score(rdm, X_clean, ybin_clean, cv=10, scoring='recall')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>ADABOOST</u> \n",
    "\n",
    "In this method equal weights are assigned to all the training examples and a base algorithm is chosen. At each step of iteration, we apply the base algorithm to the training set and increase the weights of the incorrectly classified examples. We iterate n times, each time applying base learner on the training set with updated weights. The final model is the weighted sum of the n learners. This method provides a solution to the supervised classification learning task.\n",
    "\n",
    "Boosting means building multiple models (typically of the same type) each of which learns to fix the prediction errors of a prior model in the chain. The default 50 estimators are used for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:0.71348241206\n",
      "precision:0.80440745523\n",
      "recall:0.76\n"
     ]
    }
   ],
   "source": [
    "adab = AdaBoostClassifier(n_estimators=50)\n",
    "\n",
    "print('accuracy:',end=\"\")\n",
    "print(np.mean(cross_val_score(adab, X_clean, ybin_clean, cv=10, scoring='accuracy')))\n",
    "print('precision:',end=\"\")\n",
    "print(np.mean(cross_val_score(adab, X_clean, ybin_clean, cv=10, scoring='precision')))\n",
    "print('recall:',end=\"\")\n",
    "print(np.mean(cross_val_score(adab, X_clean, ybin_clean, cv=10, scoring='recall')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly the Random forest classifier gives better accuracy, precision and recall values compared to ADABOOST. So Random Forest is the better method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Features as per ADABOOST:\n",
      "[('racePctWhite', 0.10000000000000001), ('PctKids2Par', 0.080000000000000002), ('state', 0.040000000000000001), ('agePct12t21', 0.040000000000000001), ('pctWInvInc', 0.040000000000000001), ('blackPerCap', 0.040000000000000001), ('TotalPctDiv', 0.040000000000000001), ('PctTeen2Par', 0.040000000000000001), ('communityname', 0.02), ('householdsize', 0.02)]\n"
     ]
    }
   ],
   "source": [
    "adab.fit(X_clean,ybin_clean)\n",
    "print(\"Best Features as per ADABOOST:\")\n",
    "print(sorted(list(zip(features_clean,adab.feature_importances_)),key=lambda x: -x[1])[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Features as per Random Forest:\n",
      "[('PctKids2Par', 0.073999682352805782), ('FemalePctDiv', 0.069430657633221302), ('PctPersDenseHous', 0.065748613253694049), ('PctIlleg', 0.057930310361196294), ('TotalPctDiv', 0.046009879780837745), ('racepctblack', 0.041465790123913979), ('PctTeen2Par', 0.036300954233238578), ('PctFam2Par', 0.034524010962754564), ('racePctWhite', 0.025107707977242322), ('PctHousOccup', 0.024051716134619434)]\n"
     ]
    }
   ],
   "source": [
    "rdm.fit(X_clean,ybin_clean)\n",
    "print(\"Best Features as per Random Forest:\")\n",
    "print(sorted(list(zip(features_clean,rdm.feature_importances_)),key=lambda x: -x[1])[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature importance values specify the weights of each features which when sorted in descending order provides the features aligned in decreasing order of its importance for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge clean data with temperature and military data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The merging of two dataset is done on state column. The primary data set is left joined to the tempandmil table to generate the new sample dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tempandmil = pd.read_csv('tempandmilitary.csv',sep=',', na_values=[\"\"])\n",
    "tempandmil_feat=list(tempandmil.columns[1:len(tempandmil.columns)])\n",
    "X_clean_new=X_clean.merge(tempandmil[tempandmil_feat],on='state',how='left')\n",
    "fill_NaN = Imputer(missing_values=np.nan, strategy='mean')\n",
    "X_clean_new = pd.DataFrame(fill_NaN.fit_transform(X_clean_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:0.717962311558\n",
      "precision:0.785802973192\n",
      "recall:0.7664\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(min_samples_split=20, random_state=99)\n",
    "print('accuracy:',end=\"\")\n",
    "print(np.mean(cross_val_score(dt, X_clean_new, ybin_clean, cv=10, scoring='accuracy')))\n",
    "print('precision:',end=\"\")\n",
    "print(np.mean(cross_val_score(dt, X_clean_new, ybin_clean, cv=10, scoring='precision')))\n",
    "print('recall:',end=\"\")\n",
    "print(np.mean(cross_val_score(dt, X_clean_new, ybin_clean, cv=10, scoring='recall')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without the new data appended(from above)\n",
    "\n",
    "accuracy:0.716487437186\n",
    "\n",
    "precision:0.78942069483\n",
    "\n",
    "recall:0.7648"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('PctKids2Par', 0.4462728762775991), ('racePctWhite', 0.11318607323731686), ('racePctHisp', 0.070631724672208643), ('communityname', 0.021033239187560431), ('PctSpeakEnglOnly', 0.017337167470066289), ('Air Force', 0.012953894867130522), ('HispPerCap', 0.012692628975730438), ('HousVacant', 0.012129605438407137), ('TotalPctDiv', 0.011829159070899707), ('Total Active Duty', 0.011758283512180626), ('PctYoungKids2Par', 0.01101534563750451), ('PctEmplManu', 0.010673474527515827), ('PctVacantBoarded', 0.010512322953415776), ('pctWFarmSelf', 0.010163430450656269), ('FemalePctDiv', 0.009669278225232756), ('state', 0.0085472482515365749), ('PctPopUnderPov', 0.0081378531728904802), ('PctWOFullPlumb', 0.0076534001004611198), ('PctSameState85', 0.0075994535912209515), ('PctHousOccup', 0.0073611773256511727)]\n"
     ]
    }
   ],
   "source": [
    "dt.fit(X_clean_new, ybin_clean)\n",
    "features_clean = list(crime_clean.columns[:len(crime_clean.columns)-2])\n",
    "features_clean=features_clean+tempandmil_feat\n",
    "\n",
    "print(sorted(list(zip(features_clean,dt.feature_importances_)),key=lambda x: -x[1])[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Air Force , Total Active Duty columns are top features for the decision tree created"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
